# -*- coding: utf-8 -*-
"""Group18_SportsPrediction.ipynb.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10t3Q_EqSzc28Q8qASbdC7e-dDGudCe-F

In this project, you are tasked to build a model(s) that predict a player's overall rating given the player's profile.
"""

#Importing

import pandas as pd
import numpy as np
from google.colab import drive
drive.mount('/content/drive')

"""Reading Datasets"""

players_21 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/players_21.csv')


players_21

"""DATA PROCESSING"""

#Checking columns
players_21.columns

#Dropping unwanted attributes
unwantedAttributes= ['sofifa_id','short_name', 'club_team_id', 'age', 'league_level', 'club_jersey_number', 'nationality_id', 'nation_team_id',
                      'nation_jersey_number','weak_foot', 'pace', 'player_url', 'long_name', 'dob', 'league_name',
                      'league_level', 'club_jersey_number', 'club_loaned_from', 'club_joined', 'club_contract_valid_until',
                      'nationality_id', 'nationality_name', 'nation_team_id','real_face', 'player_face_url',
                      'club_logo_url', 'club_flag_url', 'nation_logo_url', 'nation_flag_url', 'nation_position', 'club_name',
                     'power_jumping', 'goalkeeping_speed', 'ls', 'st', 'rs', 'lw','lf', 'cf', 'rf', 'rw', 'lam', 'cam', 'ram', 'lm',
                     'lcm', 'cm', 'rcm','rm', 'lwb', 'ldm', 'cdm', 'rdm', 'rwb', 'lb', 'lcb', 'cb', 'rcb', 'rb','gk', 'player_positions', 'club_position',
                     'player_tags' , 'player_traits']

#Dataset after unwanted attributes
newPlayer_21 = players_21.drop(unwantedAttributes, axis =1)

newPlayer_21.isna().any()

from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy='most_frequent')

imputed_data = imputer.fit_transform(newPlayer_21)

df = pd.DataFrame(imputed_data, columns=newPlayer_21.columns)
df

# Identify and list categorical columns
categorical_columns = df.select_dtypes(include=['object', 'category']).columns
categorical_columns

#Retrieving categorical values in each categorical columns

#preferred_foot
categorical_values = df['preferred_foot'].unique()
for value in categorical_values:
    print(value)

#work_rate
categorical_values = df['work_rate'].unique()
for value in categorical_values:
    print(value)

#body_type
categorical_values = df['body_type'].unique()
for value in categorical_values:
    print(value)

#representation of categorical data in a quantitative numerical form
#The dummies
Pdata21 = pd.get_dummies(df, columns=['preferred_foot', 'body_type', 'work_rate'])
Pdata21

"""FEATURE ENGINEERING"""

##correlation analysis
#Correlation to help check the strength of relationships
#coefficients close to zero mean that there is no linear correlation
correlation_matrix21 = Pdata21.corr()
correlation_matrix21

#selecting relevant attributes
essential_attr = ['overall','potential', 'passing', 'dribbling', 'defending', 'shooting', 'international_reputation', 'preferred_foot', 'body_type', 'work_rate']

# Dropping all columns except the specified ones
columns_to_drop = [col for col in newPlayer_21.columns if col not in essential_attr]
Player_rating_data21= Pdata21.drop(columns=columns_to_drop)
Player_rating_data21



#Dropping NAN
columns_to_check = ['overall','potential', 'passing', 'dribbling', 'defending', 'shooting', 'international_reputation', 'preferred_foot', 'body_type', 'work_rate']

# Checking for missing values in the selected columns
missing_values_essential = Player_rating_data21.isnull().sum()


#Missing values for each column
missing_values_essential

#Dealing with NAN values
#using forward fill

Player_rating_data21.fillna(method='bfill')

# Checking for missing values in the selected columns
missing_values_essential = Player_rating_data21.isnull().sum()


#Missing values for each column
missing_values_essential

#Obtaining x and y values
y_values=Player_rating_data21['overall']
x_values=Player_rating_data21.drop('overall', axis=1)

from sklearn.preprocessing import StandardScaler

#scaling the independent variables
x_values=StandardScaler().fit_transform(x_values.copy())

x_values=pd.DataFrame(x_values.copy(), columns=Player_rating_data21.drop('overall', axis=1).columns)
x_values

x_values.info()

#Test splitting
from sklearn.model_selection import train_test_split

Xtrain,Xtest,Ytrain,Ytest=train_test_split(x_values, y_values,test_size=0.2, random_state=42)



"""TRAINING MODELS"""

#cross-validation
from sklearn.model_selection import GridSearchCV

#RandomForestRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error

randomf=RandomForestRegressor()
randomf.fit(Xtrain, Ytrain)

y_pred=randomf.predict(Xtest)
y_pred

#Calculate the mean squared error (MSE)

mean_absolute_error(y_pred,Ytest)

"""Fine tuning RandomForestRegressor"""

#RandomForestRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score

model = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)

kfold = KFold(n_splits=5, shuffle=True, random_state=42)
mae_scores = -cross_val_score(model, x_values, y_values, cv=kfold, scoring='neg_mean_absolute_error')

#MAE
mean_absolute_error(y_pred,Ytest)

#XGBoost -  classification and regression tasks
import xgboost as xgb
from sklearn.model_selection import cross_val_score

# Create your XGBoost Regressor model
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)

# Perform cross-validation
scores = cross_val_score(xgb_model, x_values, y_values, cv=5, scoring='neg_mean_squared_error')

#MAE
mean_absolute_error(y_pred,Ytest)

"""Fine tuning XGBoost"""

#XGBoost -  classification and regression tasks
import xgboost as xgb
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold

model = xgb.XGBRegressor(objective="reg:squarederror", n_estimators=100, max_depth=3, learning_rate=0.1)


# Create your XGBoost Regressor model
kfold = KFold(n_splits=5, shuffle=True, random_state=42)


# Perform cross-validation
scores = cross_val_score(model, x_values, y_values, cv=kfold, scoring='neg_mean_squared_error')

#MAE
mean_absolute_error(y_pred,Ytest)

#Gradient Boost Regressors
from sklearn.ensemble import GradientBoostingRegressor
gb_regressor = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)
gb_regressor.fit(x_values, y_values)

# Make predictions
y_pred = gb_regressor.predict(Xtest)

#MAE
mean_absolute_error(y_pred,Ytest)

"""Fine Tuning Gradient Boost Regressors"""

#Gradient Boost Regressors
from sklearn.ensemble import GradientBoostingRegressor
gb_regressor = GradientBoostingRegressor(max_depth=12, n_estimators=3, learning_rate=1.0)
gb_regressor.fit(x_values, y_values)

# Make predictions
y_pred = gb_regressor.predict(Xtest)

# Evaluate the model
#Calculate the mean squared error (MSE)
#mse = mean_squared_error(Ytest, y_pred)
#print("Mean Squared Error:", mse)

#MAE
mean_absolute_error(y_pred,Ytest)

"""EVALUATION"""

from sklearn import metrics
('MAE:', metrics.mean_absolute_error(Ytest, y_pred))

"""Testing Player_22"""

players_22 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/players_22.csv')

players_22

#Dropping unwanted attributes
unwantedAttributes= ['sofifa_id','short_name', 'club_team_id', 'age', 'league_level', 'club_jersey_number', 'nationality_id', 'nation_team_id',
                      'nation_jersey_number','weak_foot', 'pace', 'player_url', 'long_name', 'dob', 'league_name',
                      'league_level', 'club_jersey_number', 'club_loaned_from', 'club_joined', 'club_contract_valid_until',
                      'nationality_id', 'nationality_name', 'nation_team_id','real_face', 'player_face_url',
                      'club_logo_url', 'club_flag_url', 'nation_logo_url', 'nation_flag_url', 'nation_position', 'club_name',
                     'power_jumping', 'goalkeeping_speed', 'ls', 'st', 'rs', 'lw','lf', 'cf', 'rf', 'rw', 'lam', 'cam', 'ram', 'lm',
                     'lcm', 'cm', 'rcm','rm', 'lwb', 'ldm', 'cdm', 'rdm', 'rwb', 'lb', 'lcb', 'cb', 'rcb', 'rb','gk', 'player_positions', 'club_position',
                     'player_tags' , 'player_traits']

#Dataset after unwanted attributes
newPlayer_22 = players_22.drop(unwantedAttributes, axis =1)

newPlayer_22.isna().any()

from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy='most_frequent')

imputed_data = imputer.fit_transform(newPlayer_22)

df = pd.DataFrame(imputed_data, columns=newPlayer_22.columns)
df

#Retrieving categorical values in each categorical columns

#preferred_foot
categorical_values = df['preferred_foot'].unique()
for value in categorical_values:
    print(value)#Retrieving categorical values in each categorical columns

#work_rate
categorical_values = df['work_rate'].unique()
for value in categorical_values:
    print(value)

#body_type
categorical_values = df['body_type'].unique()
for value in categorical_values:
    print(value)

#representation of categorical data in a quantitative numerical form
#The dummies
Pdata22 = pd.get_dummies(df, columns=['preferred_foot', 'body_type', 'work_rate'])
Pdata22

##correlation analysis
#Correlation to help check the strength of relationships
#coefficients close to zero mean that there is no linear correlation
correlation_matrix22 = Pdata22.corr()
correlation_matrix22

#selecting relevant attributes
essential_attr = ['overall','potential', 'passing', 'dribbling', 'defending', 'shooting', 'international_reputation', 'preferred_foot', 'body_type', 'work_rate']

# Dropping all columns except the specified ones
columns_to_drop = [col for col in newPlayer_22.columns if col not in essential_attr]
Player_rating_data22= Pdata22.drop(columns=columns_to_drop)
Player_rating_data22

#Dropping NAN
columns_to_check = ['overall','potential', 'passing', 'dribbling', 'defending', 'shooting', 'international_reputation', 'preferred_foot', 'body_type', 'work_rate']

# Checking for missing values in the selected columns
missing_values_essential = Player_rating_data22.isnull().sum()


#Missing values for each column
missing_values_essential

#Dealing with NAN values
#using forward fill

Player_rating_data22.fillna(method='bfill')

Player_rating_data21.columns

#Obtaining x and y values
y_values=Player_rating_data22['overall']
x_values=Player_rating_data22.drop('overall', axis=1)

from sklearn.preprocessing import StandardScaler

#scaling the independent variables
x_values=StandardScaler().fit_transform(x_values.copy())

x_values=pd.DataFrame(x_values.copy(), columns=Player_rating_data22.drop('overall', axis=1).columns)
x_values

#Testing players_22 Dataset

#Test splitting
from sklearn.model_selection import train_test_split

Xtrain,Xtest,Ytrain,Ytest=train_test_split(x_values, y_values,test_size=0.2, random_state=42)

#Testing with XGBoost

from sklearn.metrics import mean_absolute_error

y_pred = gb_regressor.predict(Xtest)
mae = mean_absolute_error(y_pred,Ytest)

print(mae)

results = pd.DataFrame({'Predicted': np.round(y_pred), 'Actual': Ytest})

results

# pickling the model
import pickle
pickle_out = open("gb_regressor.pkl", "wb")
pickle.dump(gb_regressor, pickle_out)
pickle_out.close()

import sklearn
sklearn.__version__

"""Deployment"""